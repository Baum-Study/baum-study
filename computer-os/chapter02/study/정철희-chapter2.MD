## 1. 0과 1로 숫자를 표현하는 방법

### 1-1 정보 단위

이전 챕터에서 컴퓨터는 0과 1로 이루어진 명령어와 데이터만을 읽을 수 있다고 배웠습니다.

0과 1은 bit라는 단위로 표현이 가능합니다.

8개의 bit를 하나로 묶은 단위를 byte라고 합니다.

주의할 점은 byte부터는 1024개가 아닌 1000개씩 묶어야 단위가 변경이 됩니다.

1000 byte는 1KB, 1000KB는 1MB, 1000MB는 1GB, 1000GB는 1TB 입니다.

우리는 CPU가 한 번에 처리를 할 수 있는 워드라는 단위도 알고 있어야합니다.
`intel의 x86 cpu는 32비트 워드 cpu, 64비트 워드 cpu가 있습니다.`

`과연 처리할 수 있는 단위가 큰 64비트 워드가 무조건 좋을까요?`

`번외로 Database에 boolean 값을 넣을때 bit type이 존재하기도 합니다.`

### 1-2 이진법

0과 1로 구성된 이진수는 앞에 0b를 붙히거나 뒤에 (2) 라는 아래첨자를 붙히지 않는다면 8진수, 10진수들과 구분을 할 수 없습니다.

아래첨자를 붙히는 방식은 수학적으로 표기할 때 사용하는 방식입니다.

그래서 컴퓨터는 0b라는 구분 코드를 붙혀야 비로소 2진수임을 이해합니다.

`0b, 0x와 같은 구분 코드를 붙히는 경우를 본 적 있나요?`

### 1-3 음수

0과 1로 이루워진 bit로 어떻게 음수를 표현할까요?

-를 붙히면 편하겠지만 컴퓨터는 그러지 않습니다.

8비트 중 맨 앞에 1비트가 1이면 마이너스를 의미하게 됩니다. (flag 값)
`kotlin은 ULong이라는 음수가 없는 타입이 있습니다.`

### 1-4 십육진법

컴퓨터가 매번 2진수로 처리를 하게되면 엄청나게 많은 양의 데이터를 읽어들여야합니다.

16진수를 사용하게 되면 2진수로 변환이 매우 쉽기 때문에  컴퓨터는 2의 배수인 16진수로 자주 표기합니다.

변환 방법은 각자 공부했기 때문에 생략하겠습니다.ㅎ

## 2. 0과 1로 문자를 표현하는 방법

### 2-1  문자 집합과 인코딩

어떻게 컴퓨터는 0과 1로 된 데이터를 우리에게 읽을 수 있게 문자형태로 보여줄까요?

정답은 컴퓨터는 매번 인코딩과 디코딩이라는 변환 작업을 통해 우리가 읽을 수 있게 보여줍니다.

프로그래밍을 하다보면 인코딩, 디코딩이라는 단어들을 정말 많이 접할 수 있습니다.
`우리말로 암호화, 복호화라고도 하는 이 기능을 사용해본적 있나요?`

### 2-2 아스키 코드

ASCII 코드는 초기 컴퓨터 통신에서 문자를 표현하는 표준 방법입니다.

하지만 현대로 오면서 컴퓨터는 영어, 특수문자, 제어문자로 이루어진 아스키 코드만으로는 표기 할 때 문제가 생깁니다.

그래서 1비트가 늘어난 extended ASCII가 등장했지만 그래도 표기하는데 많은 어려움이 있습니다.

### 2-3 EUC-KR

EUC-KR란 한글과 영어를 인코딩 할 수 있는 인코딩 방식입니다.

가장 초기에 나온 인코딩 방식이라 한글을 완전히 표기하지 못했습니다.

때문에 전산상에서 글자가 깨지는 문제가 발생하기도 했습니다.

### 2-4 유니코드와 UTF-8

유니코드는 모든 나라의 문자 집합과 인코딩 방식을 통일하는 것을 목표로 만들어졌습니다.

UTF-8 방식은 유니코드를 1바이트부터 4바이트까지 가변 길이 인코딩 방식을 통해 인코딩 한 것을 의미합니다.

유니코드는 부여된 값의 범위에 따라 인코딩 결과가 달라집니다.

한글은 유니코드 내에 0000 0000 0100 0000부터 0000 0100 0000 0000 사이에 존재하기 때문에 3바이트로 표현됩니다.
`UTF-8과 UTF-16의 차이점은 무엇일까요?`
`때문에 한글은 글자 하나하나가 3바이트입니다.`